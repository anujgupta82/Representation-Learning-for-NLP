{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# BiGram Model\n",
    "\n",
    "----------\n",
    "\n",
    "#### Built on lines of  &nbsp;&nbsp;&nbsp;&nbsp;   [representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb](http://localhost:8888/notebooks/representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb)\n",
    "\n",
    "### Added \n",
    "    1) tensorboard network visualizations   \n",
    "    2) tensorboard loss visualizations\n",
    "    3) similar words to words in validation data\n",
    "\n",
    "#### Author : Anuj\n",
    "\n",
    "#### Uses Wikipedia Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../../Utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from readWikiData import get_wikipedia_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the data file - map tokens to Ids, convert data to Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_data(n_vocab_=1000):\n",
    "    sentences, word2idx = get_wikipedia_data(n_vocab=n_vocab_, n_files=10, by_paragraph=True)\n",
    "    training_data = []\n",
    "    vocab_size = len(word2idx)\n",
    "    for sentence in sentences:\n",
    "        for elem1, elem2 in zip(sentence[:-1], sentence[1:]):\n",
    "            training_data.append((elem1, elem2))\n",
    "    \n",
    "    # this destroys the order of words in a wondow but for bigram its harmless\n",
    "    # all we want is - pair of all bigrams\n",
    "    training_data = list(set(training_data))   \n",
    "    \n",
    "    idx2word = {v:k for k, v in word2idx.iteritems()}\n",
    "    return len(word2idx), training_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size, training_data, word2idx, idx2word = get_wiki_data(n_vocab_=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<type 'list'>\n",
      "1659944\n"
     ]
    }
   ],
   "source": [
    "print vocab_size\n",
    "print type(training_data)\n",
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print len(word2idx.keys())\n",
    "print len(idx2word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build validation set - randomly choose 100 keys from idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly pick some validation words from data\n",
    "\n",
    "validation_size = 32\n",
    "#validation_set = random.sample(idx2word.keys(), validation_size)\n",
    "validation_set = random.sample(idx2word.keys(), validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1375, 5115, 8555, 8342, 5003, 4755, 7461, 5688, 299, 2480, 9753, 626, 5782, 3637, 129, 4803, 7629, 8293, 8195, 9757, 7001, 4702, 3200, 7030, 5530, 4713, 8912, 7397, 1092, 9656, 8360, 4639]\n",
      "['1992', 'funded', 'pennant', 'companions', 'minimal', 'lights', 'lattice', 'releasing', 'rather', 'peninsula', 'altruism', 'come', 'cruise', 'anxiety', 'include', 'emission', 'mountainous', 'codex', 'alcoholism', 'clone', 'cheaper', 'virus', 'fund', 'invested', 'overcome', 'innovation', 'enforce', 'nowadays', 'effective', 'jew', 'rooted', 'solely']\n"
     ]
    }
   ],
   "source": [
    "print validation_set\n",
    "print [idx2word[index] for index in validation_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contains list of pairs that have already been selected\n",
    "bucket_list = []\n",
    "\n",
    "def getNextBatch(bi_grams_, batch_size=1000):\n",
    "    \n",
    "    global bucket_list\n",
    "    \n",
    "    # list of possible pairs to pick from\n",
    "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
    "    \n",
    "    # once you exhaust the possible pais, reset\n",
    "    if len(docs_ids_to_select) < batch_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = bi_grams_\n",
    "        \n",
    "    # Initialize two variables \n",
    "    train_X = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # pick a random chunks of pairs \n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    # Iterate threw all the docs \n",
    "    for item in random_docs:\n",
    "        train_X[index] = item[0]\n",
    "        train_label[index] = item[1]  \n",
    "        index += 1\n",
    "        \n",
    "    #flatten list of lists to a single list\n",
    "    train_X = list(itertools.chain(*train_X))\n",
    "    train_label = list(itertools.chain(*train_label))\n",
    "            \n",
    "    return train_X, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, Y = getNextBatch(bi_grams_=training_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 51873\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_batches = len(training_data)/batch_size\n",
    "\n",
    "print \"Number of batches = %d\" %num_batches\n",
    "\n",
    "\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='X_var')\n",
    "Y = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='Y_var')\n",
    "valid_X = tf.Variable(validation_set, dtype=tf.int32, name='X_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oh = tf.one_hot(indices=X, depth=vocab_size, name='Converting_Y_to_Y_oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32,)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print X.get_shape()\n",
    "print Y.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer_1 = tf.Variable(tf.truncated_normal(\n",
    "    shape=(vocab_size, embedding_dims),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Embeddings_Matrix\") \n",
    "embeded = tf.nn.embedding_lookup(embedding_layer_1, ids=X, name=\"Embedding_LookUp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(128)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax weights, bias\n",
    "W = tf.Variable(tf.truncated_normal(\n",
    "    shape=(embedding_dims, vocab_size),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Softmax_Weights_Matrix\")\n",
    "b = tf.Variable(tf.zeros(shape=(vocab_size,)), name=\"Softmax_Bias_Vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.add(tf.matmul(embeded, W, name=\"WX\"), b, name=\"WX_plus_b\")\n",
    "\n",
    "#logits = tf.add(tf.matmul(embed, softmax_weights, name=\"WX\"), softmax_bias, name=\"WX_plus_b\")\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_oh, name=\"Compute_Loss\")\n",
    "#mean_loss = tf.reduce_mean(loss)\n",
    "mean_loss = tf.reduce_mean(loss, name=\"Compute_mean_loss\")\n",
    "\n",
    "tf.summary.scalar(\"mean_loss\", mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print logits.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5, name=\"Optimizer\").minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute L2 norm for cosine similarity\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer_1), axis=1, keep_dims=True))\n",
    "normalised_embeddings = embedding_layer_1 / norm\n",
    "\n",
    "# get validation set embeddings\n",
    "validation_data_embeddings = tf.nn.embedding_lookup(normalised_embeddings, ids=valid_X, name=\"validation_embeddings_lookup\")\n",
    "\n",
    "# similarity score of validation embeddings w.r.t normalised= dot product between validation_data_embeddings and mornalised embeddings\n",
    "similarity = tf.matmul(validation_data_embeddings, tf.transpose(normalised_embeddings))  # C.A = C x transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 51873\n",
      "Number of epochs = 20\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 20\n",
    "LOG_DIR = './bigram_wiki_chk_pts'\n",
    "\n",
    "print \"Number of batches = %d\" %num_batches\n",
    "print \"Number of epochs = %d\" %num_of_epochs\n",
    "\n",
    "\n",
    "validation_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo params \n",
    "\n",
    "num_of_epochs = 5\n",
    "num_batches = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n",
      "\n",
      "step_id = 0\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 32.895508\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 32.895508\n",
      "\n",
      "\t Nearest to 1992 :  oxygen, path, history, critics, 1846, furniture, chapel, decree,\n",
      "\t Nearest to funded :  expectation, blockade, sudan, ed, 44, resident, richards, obvious,\n",
      "\t Nearest to pennant :  defend, lung, shallow, educated, mind, contributing, geoffrey, reign,\n",
      "\t Nearest to companions :  front, developers, poetic, fraud, earliest, lincolns, 1936, odd,\n",
      "\t Nearest to minimal :  aviation, africans, sexual, them, crossbow, 300000, relied, vienna,\n",
      "\t Nearest to lights :  volunteer, valve, darkness, 1924, legs, counsel, alike, underground,\n",
      "\t Nearest to lattice :  relying, make, donald, tried, acupuncture, coma, churchill, coordinated,\n",
      "\t Nearest to releasing :  dallas, assert, jewelry, tended, knight, concentrations, undergraduate, afonso,\n",
      "step_id = 1\n",
      "step_id = 2\n",
      "step_id = 3\n",
      "step_id = 4\n",
      "step_id = 5\n",
      "step_id = 6\n",
      "step_id = 7\n",
      "step_id = 8\n",
      "step_id = 9\n",
      "step_id = 10\n",
      "step_id = 11\n",
      "step_id = 12\n",
      "step_id = 13\n",
      "step_id = 14\n",
      "step_id = 15\n",
      "step_id = 16\n",
      "step_id = 17\n",
      "step_id = 18\n",
      "step_id = 19\n",
      "step_id = 20\n",
      "step_id = 21\n",
      "step_id = 22\n",
      "step_id = 23\n",
      "step_id = 24\n",
      "step_id = 25\n",
      "step_id = 26\n",
      "step_id = 27\n",
      "step_id = 28\n",
      "step_id = 29\n",
      "step_id = 30\n",
      "step_id = 31\n",
      "step_id = 32\n",
      "step_id = 33\n",
      "step_id = 34\n",
      "step_id = 35\n",
      "step_id = 36\n",
      "step_id = 37\n",
      "step_id = 38\n",
      "step_id = 39\n",
      "step_id = 40\n",
      "step_id = 41\n",
      "step_id = 42\n",
      "step_id = 43\n",
      "step_id = 44\n",
      "step_id = 45\n",
      "step_id = 46\n",
      "step_id = 47\n",
      "step_id = 48\n",
      "step_id = 49\n",
      "\n",
      "For epoch = 0, Av loss = 31.834616\n",
      "step_id = 50\n",
      "\n",
      "For epoch = 1, batch id = 0, batch loss = 31.480677\n",
      "\n",
      "\n",
      "For epoch = 1, batch id = 0, batch loss = 31.480677\n",
      "\n",
      "\t Nearest to 1992 :  oxygen, path, history, critics, 1846, furniture, chapel, decree,\n",
      "\t Nearest to funded :  expectation, blockade, sudan, ed, 44, resident, richards, obvious,\n",
      "\t Nearest to pennant :  defend, lung, shallow, educated, mind, contributing, geoffrey, reign,\n",
      "\t Nearest to companions :  front, developers, poetic, earliest, fraud, lincolns, 1936, hymns,\n",
      "\t Nearest to minimal :  aviation, africans, sexual, them, crossbow, 300000, relied, vienna,\n",
      "\t Nearest to lights :  volunteer, valve, darkness, 1924, legs, counsel, alike, underground,\n",
      "\t Nearest to lattice :  relying, make, donald, tried, acupuncture, coma, churchill, coordinated,\n",
      "\t Nearest to releasing :  dallas, assert, jewelry, tended, knight, concentrations, undergraduate, afonso,\n",
      "step_id = 51\n",
      "step_id = 52\n",
      "step_id = 53\n",
      "step_id = 54\n",
      "step_id = 55\n",
      "step_id = 56\n",
      "step_id = 57\n",
      "step_id = 58\n",
      "step_id = 59\n",
      "step_id = 60\n",
      "step_id = 61\n",
      "step_id = 62\n",
      "step_id = 63\n",
      "step_id = 64\n",
      "step_id = 65\n",
      "step_id = 66\n",
      "step_id = 67\n",
      "step_id = 68\n",
      "step_id = 69\n",
      "step_id = 70\n",
      "step_id = 71\n",
      "step_id = 72\n",
      "step_id = 73\n",
      "step_id = 74\n",
      "step_id = 75\n",
      "step_id = 76\n",
      "step_id = 77\n",
      "step_id = 78\n",
      "step_id = 79\n",
      "step_id = 80\n",
      "step_id = 81\n",
      "step_id = 82\n",
      "step_id = 83\n",
      "step_id = 84\n",
      "step_id = 85\n",
      "step_id = 86\n",
      "step_id = 87\n",
      "step_id = 88\n",
      "step_id = 89\n",
      "step_id = 90\n",
      "step_id = 91\n",
      "step_id = 92\n",
      "step_id = 93\n",
      "step_id = 94\n",
      "step_id = 95\n",
      "step_id = 96\n",
      "step_id = 97\n",
      "step_id = 98\n",
      "step_id = 99\n",
      "\n",
      "For epoch = 1, Av loss = 28.933959\n",
      "step_id = 100\n",
      "\n",
      "For epoch = 2, batch id = 0, batch loss = 25.417118\n",
      "\n",
      "\n",
      "For epoch = 2, batch id = 0, batch loss = 25.417118\n",
      "\n",
      "\t Nearest to 1992 :  oxygen, path, history, critics, 1846, furniture, chapel, decree,\n",
      "\t Nearest to funded :  expectation, blockade, sudan, ed, 44, resident, richards, obvious,\n",
      "\t Nearest to pennant :  defend, lung, shallow, educated, mind, contributing, geoffrey, reign,\n",
      "\t Nearest to companions :  front, developers, poetic, earliest, fraud, lincolns, 1936, hymns,\n",
      "\t Nearest to minimal :  aviation, africans, them, sexual, crossbow, 300000, relied, vienna,\n",
      "\t Nearest to lights :  volunteer, valve, darkness, 1924, legs, counsel, alike, underground,\n",
      "\t Nearest to lattice :  relying, make, donald, tried, acupuncture, coma, churchill, coordinated,\n",
      "\t Nearest to releasing :  dallas, assert, jewelry, tended, knight, concentrations, undergraduate, afonso,\n",
      "step_id = 101\n",
      "step_id = 102\n",
      "step_id = 103\n",
      "step_id = 104\n",
      "step_id = 105\n",
      "step_id = 106\n",
      "step_id = 107\n",
      "step_id = 108\n",
      "step_id = 109\n",
      "step_id = 110\n",
      "step_id = 111\n",
      "step_id = 112\n",
      "step_id = 113\n",
      "step_id = 114\n",
      "step_id = 115\n",
      "step_id = 116\n",
      "step_id = 117\n",
      "step_id = 118\n",
      "step_id = 119\n",
      "step_id = 120\n",
      "step_id = 121\n",
      "step_id = 122\n",
      "step_id = 123\n",
      "step_id = 124\n",
      "step_id = 125\n",
      "step_id = 126\n",
      "step_id = 127\n",
      "step_id = 128\n",
      "step_id = 129\n",
      "step_id = 130\n",
      "step_id = 131\n",
      "step_id = 132\n",
      "step_id = 133\n",
      "step_id = 134\n",
      "step_id = 135\n",
      "step_id = 136\n",
      "step_id = 137\n",
      "step_id = 138\n",
      "step_id = 139\n",
      "step_id = 140\n",
      "step_id = 141\n",
      "step_id = 142\n",
      "step_id = 143\n",
      "step_id = 144\n",
      "step_id = 145\n",
      "step_id = 146\n",
      "step_id = 147\n",
      "step_id = 148\n",
      "step_id = 149\n",
      "\n",
      "For epoch = 2, Av loss = 26.234670\n",
      "step_id = 150\n",
      "\n",
      "For epoch = 3, batch id = 0, batch loss = 24.579496\n",
      "\n",
      "\n",
      "For epoch = 3, batch id = 0, batch loss = 24.579496\n",
      "\n",
      "\t Nearest to 1992 :  oxygen, path, history, critics, 1846, furniture, chapel, decree,\n",
      "\t Nearest to funded :  expectation, blockade, sudan, ed, 44, resident, richards, obvious,\n",
      "\t Nearest to pennant :  defend, lung, shallow, educated, mind, contributing, geoffrey, reign,\n",
      "\t Nearest to companions :  front, developers, poetic, earliest, fraud, lincolns, 1936, hymns,\n",
      "\t Nearest to minimal :  aviation, africans, them, sexual, crossbow, 300000, relied, vienna,\n",
      "\t Nearest to lights :  volunteer, valve, darkness, 1924, legs, counsel, alike, underground,\n",
      "\t Nearest to lattice :  relying, make, donald, tried, acupuncture, coma, coordinated, churchill,\n",
      "\t Nearest to releasing :  dallas, assert, jewelry, tended, knight, concentrations, undergraduate, afonso,\n",
      "step_id = 151\n",
      "step_id = 152\n",
      "step_id = 153\n",
      "step_id = 154\n",
      "step_id = 155\n",
      "step_id = 156\n",
      "step_id = 157\n",
      "step_id = 158\n",
      "step_id = 159\n",
      "step_id = 160\n",
      "step_id = 161\n",
      "step_id = 162\n",
      "step_id = 163\n",
      "step_id = 164\n",
      "step_id = 165\n",
      "step_id = 166\n",
      "step_id = 167\n",
      "step_id = 168\n",
      "step_id = 169\n",
      "step_id = 170\n",
      "step_id = 171\n",
      "step_id = 172\n",
      "step_id = 173\n",
      "step_id = 174\n",
      "step_id = 175\n",
      "step_id = 176\n",
      "step_id = 177\n",
      "step_id = 178\n",
      "step_id = 179\n",
      "step_id = 180\n",
      "step_id = 181\n",
      "step_id = 182\n",
      "step_id = 183\n",
      "step_id = 184\n",
      "step_id = 185\n",
      "step_id = 186\n",
      "step_id = 187\n",
      "step_id = 188\n",
      "step_id = 189\n",
      "step_id = 190\n",
      "step_id = 191\n",
      "step_id = 192\n",
      "step_id = 193\n",
      "step_id = 194\n",
      "step_id = 195\n",
      "step_id = 196\n",
      "step_id = 197\n",
      "step_id = 198\n",
      "step_id = 199\n",
      "\n",
      "For epoch = 3, Av loss = 25.328911\n",
      "step_id = 200\n",
      "\n",
      "For epoch = 4, batch id = 0, batch loss = 25.002098\n",
      "\n",
      "\n",
      "For epoch = 4, batch id = 0, batch loss = 25.002098\n",
      "\n",
      "\t Nearest to 1992 :  oxygen, path, history, critics, 1846, furniture, chapel, decree,\n",
      "\t Nearest to funded :  expectation, blockade, sudan, ed, 44, resident, richards, obvious,\n",
      "\t Nearest to pennant :  defend, lung, shallow, educated, mind, contributing, geoffrey, reign,\n",
      "\t Nearest to companions :  front, developers, poetic, earliest, fraud, lincolns, 1936, hymns,\n",
      "\t Nearest to minimal :  aviation, africans, them, sexual, crossbow, 300000, relied, vienna,\n",
      "\t Nearest to lights :  volunteer, valve, darkness, 1924, legs, counsel, alike, underground,\n",
      "\t Nearest to lattice :  relying, make, donald, tried, acupuncture, coma, coordinated, churchill,\n",
      "\t Nearest to releasing :  dallas, assert, jewelry, tended, knight, concentrations, undergraduate, afonso,\n",
      "step_id = 201\n",
      "step_id = 202\n",
      "step_id = 203\n",
      "step_id = 204\n",
      "step_id = 205\n",
      "step_id = 206\n",
      "step_id = 207\n",
      "step_id = 208\n",
      "step_id = 209\n",
      "step_id = 210\n",
      "step_id = 211\n",
      "step_id = 212\n",
      "step_id = 213\n",
      "step_id = 214\n",
      "step_id = 215\n",
      "step_id = 216\n",
      "step_id = 217\n",
      "step_id = 218\n",
      "step_id = 219\n",
      "step_id = 220\n",
      "step_id = 221\n",
      "step_id = 222\n",
      "step_id = 223\n",
      "step_id = 224\n",
      "step_id = 225\n",
      "step_id = 226\n",
      "step_id = 227\n",
      "step_id = 228\n",
      "step_id = 229\n",
      "step_id = 230\n",
      "step_id = 231\n",
      "step_id = 232\n",
      "step_id = 233\n",
      "step_id = 234\n",
      "step_id = 235\n",
      "step_id = 236\n",
      "step_id = 237\n",
      "step_id = 238\n",
      "step_id = 239\n",
      "step_id = 240\n",
      "step_id = 241\n",
      "step_id = 242\n",
      "step_id = 243\n",
      "step_id = 244\n",
      "step_id = 245\n",
      "step_id = 246\n",
      "step_id = 247\n",
      "step_id = 248\n",
      "step_id = 249\n",
      "\n",
      "For epoch = 4, Av loss = 23.965704\n",
      "Model saved in file: ./bigram_wiki_chk_pts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# A SIMPLE saver() to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # writer to write graph to tensorboard\n",
    "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    print \"initialised\\n\"\n",
    "\n",
    "    for epoch_id in range(num_of_epochs):\n",
    "\n",
    "        av_batch_loss = 0\n",
    "\n",
    "        for batch_id in range(num_batches):\n",
    "\n",
    "            X_, Y_ = getNextBatch(bi_grams_=training_data, batch_size=batch_size)\n",
    "\n",
    "            feed_dict = {}\n",
    "            feed_dict[X] = X_\n",
    "            feed_dict[Y] = Y_\n",
    "\n",
    "            batch_loss, _, summary = sess.run([mean_loss, optimizer, summary_op], feed_dict=feed_dict)\n",
    "            \n",
    "            #writer.add_summary(batch_loss, epoch) \n",
    "            step_id = epoch_id * num_batches + batch_id\n",
    "            print \"step_id = %d\" %step_id\n",
    "            writer.add_summary(summary, global_step=step_id)\n",
    "\n",
    "            av_batch_loss += batch_loss\n",
    "            \n",
    "            if batch_id % 500 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "            \n",
    "            if batch_id % 1000 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "                \n",
    "                #print validation data\n",
    "                sim = similarity.eval() # compute similarity\n",
    "                \n",
    "                #iterate over each validation example\n",
    "                \n",
    "                for i in range(validation_size):\n",
    "                    word = idx2word[validation_set[i]]\n",
    "                    top_k = 8\n",
    "                    # sort indexes and pick top k. we take 1:top_k+1 since 0th top pick will the same word itself\n",
    "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    #nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    log = '\\t Nearest to %s : ' %word\n",
    "                    for k in range(top_k):\n",
    "                        nearest_word = idx2word[nearest[k]]\n",
    "                        log = '%s %s,' %(log, nearest_word)\n",
    "                    print log        \n",
    "\n",
    "        print \"\\nFor epoch = %d, Av loss = %f\" %(epoch_id, av_batch_loss/num_batches)\n",
    "        \n",
    "        #batch.reset()\n",
    "        \n",
    "    save_path = saver.save(sess, LOG_DIR)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot the Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_epochs = 5\n",
    "num_batches = 50\n",
    "\n",
    "for epoch_id in range(num_of_epochs):\n",
    "    for batch_id in range(num_batches):\n",
    "        \n",
    "        step_id = epoch_id * num_batches + batch_id\n",
    "        \n",
    "        print step_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
